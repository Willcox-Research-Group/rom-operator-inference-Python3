{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `opinf.lstsq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ".. automodule:: opinf.lstsq\n",
    "\n",
    ".. currentmodule:: opinf.lstsq\n",
    "\n",
    ".. autosummary::\n",
    "   :toctree: _autosummaries\n",
    "   :nosignatures:\n",
    "\n",
    "   SolverTemplate\n",
    "   PlainSolver\n",
    "   L2Solver\n",
    "   L2DecoupledSolver\n",
    "   TikhonovSolver\n",
    "   TikhonovDecoupledSolver\n",
    "   TruncatedSVDSolver\n",
    "   TotalLeastSquaresSolver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Overview\n",
    ":class: note\n",
    "\n",
    "- `opinf.lstsq` classes are solve Operator Inference regression problems.\n",
    "- `opinf.models` classes handle the construction of the regression matrices from snapshot data, pass these matrices to the solver's [`fit()`](SolverTemplate.fit) method, call the solver's [`solve()`](SolverTemplate.solve) method, and interpret the solution in the context of the model structure.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Example Data\n",
    ":class: tip\n",
    "The examples on this page use data matrices composed of compressed heat flow data from [tutorial 2](../tutorials/heat_equation.ipynb).\n",
    "You can [download the data here](https://github.com/Willcox-Research-Group/rom-operator-inference-Python3/raw/data/lstsq_example.npz) to repeat the demonstration.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "import opinf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Inference Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operator Inference uses data to learn the entries of an $r \\times d$ *operator matrix* $\\Ohat$ by solving a regression problem, stated generally as\n",
    "\n",
    "::::{margin}\n",
    ":::{admonition} What is $\\Z$?\n",
    "For continuous models (systems of ordinary differential equations), $\\Z$ consists of the time derivatives of the snapshots; for discrete models (discrete dynamical systems), $\\Z$ also contains state snapshots.\n",
    ":::\n",
    "::::\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{find}\\quad\\Ohat\\quad\\text{such that}\\quad\n",
    "    \\Z \\approx \\Ohat\\D\\trp\n",
    "    \\quad\\Longleftrightarrow\\quad\n",
    "    \\D\\Ohat\\trp \\approx \\Z\\trp,\n",
    "\\end{aligned}\n",
    "$$ (eq:lstsq:general)\n",
    "\n",
    "where $\\D$ is the $k \\times d$ *data matrix* formed from state and input snapshots and $\\Z$ is the $r \\times k$ matrix of left-hand side data.\n",
    "\n",
    "This module defines classes for solving different variations of the regression {eq}`eq:lstsq:general` given the data matrices $\\D$ and $\\Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "::::{admonition} Example\n",
    ":class: tip\n",
    "\n",
    "Suppose we want to construct a linear time-invariant (LTI) system,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ddt\\qhat(t)\n",
    "    = \\Ahat\\qhat(t) + \\Bhat\\u(t),\n",
    "    \\qquad\n",
    "    \\Ahat\\in\\RR^{r \\times r},\n",
    "    ~\n",
    "    \\Bhat\\in\\RR^{r \\times m}.\n",
    "\\end{align}\n",
    "$$ (eq:lstsq:ltiexample)\n",
    "\n",
    "The operator matrix is $\\Ohat = [~\\Ahat~~\\Bhat~]\\in\\RR^{r \\times d}$ with column dimension $d = r + m$.\n",
    "To learn $\\Ohat$ with Operator Inference, we need data for $\\qhat(t)$, $\\u(t)$, and $\\ddt\\qhat(t)$.\n",
    "For $j = 0, \\ldots, k-1$, let\n",
    "\n",
    "- $\\qhat_{j}\\in\\RR^r$ be a measurement of the (reduced) state at time $t_{j}$,\n",
    "- $\\dot{\\qhat}_{j} = \\ddt\\qhat(t)\\big|_{t=t_{j}} \\in \\RR^r$ be the time derivative of the state at time $t_{j}$, and\n",
    "- $\\u_{j} = \\u(t_j) \\in \\RR^m$ be the input at time $t_{j}$.\n",
    "\n",
    "In this case, the data matrix $\\D$ is given by $\\D = [~\\Qhat\\trp~~\\U\\trp~]\\in\\RR^{k \\times d}$, where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Qhat = \\left[\\begin{array}{ccc}\n",
    "        & & \\\\\n",
    "        \\qhat_0 & \\cdots & \\qhat_{k-1}\n",
    "        \\\\ & &\n",
    "    \\end{array}\\right]\n",
    "    \\in \\RR^{r\\times k},\n",
    "    \\qquad\n",
    "    \\U = \\left[\\begin{array}{ccc}\n",
    "        & & \\\\\n",
    "        \\u_0 & \\cdots & \\u_{k-1}\n",
    "        \\\\ & &\n",
    "    \\end{array}\\right]\n",
    "    \\in \\RR^{m \\times k}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The left-hand side data is $\\Z = \\dot{\\Qhat} = [~\\dot{\\qhat}_0~~\\cdots~~\\dot{\\qhat}_{k-1}~]\\in\\RR^{r\\times k}$.\n",
    "\n",
    ":::{dropdown} Derivation\n",
    "We seek $\\Ahat$ and $\\Bhat$ such that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\dot{\\qhat}_{j}\n",
    "    \\approx \\Ahat\\qhat_j + \\Bhat\\u_j,\n",
    "    \\qquad j = 0, \\ldots, k-1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the snapshot matrices $\\Qhat$, $\\U$, and $\\dot{\\Qhat}$ defined above, we want\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\dot{\\Qhat}\n",
    "    \\approx \\Ahat\\Qhat + \\Bhat\\U\n",
    "    = [~\\Ahat~~\\Bhat~]\\left[\\begin{array}{c} \\Qhat \\\\ \\U \\end{array}\\right],\n",
    "    \\quad\\text{or}\n",
    "    \\\\\n",
    "    [~\\Qhat\\trp~~\\U\\trp~][~\\Ahat~~\\Bhat~]\\trp \\approx \\dot{\\Qhat}\\trp,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is $\\D\\Ohat\\trp \\approx \\Z\\trp$.\n",
    "\n",
    "More precisely, a regression problem for $\\Ohat$ with respect to the data triples $(\\qhat_j, \\u_j, \\dot{\\qhat}_j)$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\Ahat,\\Bhat}\\sum_{j=0}^{k-1}\\left\\|\n",
    "        \\Ahat\\qhat_j + \\Bhat\\u_j - \\dot{\\qhat}_j\n",
    "    \\right\\|_{2}^{2}\n",
    "    &= \\argmin_{\\Ahat,\\Bhat}\\left\\|\n",
    "        \\Ahat\\Qhat + \\Bhat\\U - \\dot{\\Qhat}\n",
    "    \\right\\|_{F}^{2}\n",
    "    \\\\\n",
    "    &= \\argmin_{\\Ahat,\\Bhat}\\left\\|\n",
    "        [~\\Ahat~~\\Bhat~]\\left[\\begin{array}{c} \\Qhat \\\\ \\U \\end{array}\\right] - \\Z\n",
    "    \\right\\|_{F}^{2}\n",
    "    \\\\\n",
    "    &= \\argmin_{\\Ahat,\\Bhat}\\left\\|\n",
    "        [~\\Qhat\\trp~~\\U\\trp~][~\\Ahat~~\\Bhat~]\\trp - \\Z\\trp\n",
    "    \\right\\|_{F}^{2},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is $\\argmin_{\\Ohat}\\|\\D\\Ohat\\trp - \\Z\\trp\\|_F^2$.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often, we pose {eq}`eq:lstsq:general` as a linear least-squares regression,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\Ohat} \\|\\D\\Ohat\\trp - \\Z\\trp\\|_F^2.\n",
    "\\end{aligned}\n",
    "$$ (eq:lstsq:plain)\n",
    "\n",
    "Note that the matrix least-squares problem {eq}`eq:lstsq:plain` decouples into $r$ independent vector least-squares problems, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\ohat_i} \\|\\D\\ohat_i - \\z_i\\|_2^2,\n",
    "    \\quad i = 1, \\ldots, r,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\ohat_i$ and $\\z_i$ are the $i$-th rows of $\\Ohat$ and $\\Z$, respectively.\n",
    "\n",
    "The {class}`PlainSolver` class solves {eq}`eq:lstsq:plain` without any additional terms.\n",
    "This is the default solver used if another solver is not specified in the constructor of an {mod}`opinf.models` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and extract the example data.\n",
    "data = np.load(\"lstsq_example.npz\")\n",
    "D = data[\"data_matrix\"]\n",
    "Z = data[\"lhs_matrix\"]\n",
    "\n",
    "print(f\"{D.shape=}, {Z.shape=}\")\n",
    "\n",
    "# Infer problem dimensions from the data.\n",
    "k, d = D.shape\n",
    "r = Z.shape[0]\n",
    "\n",
    "print(f\"{r=}, {d=}, {k=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = opinf.lstsq.PlainSolver()\n",
    "print(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the solver to the regression data.\n",
    "solver.fit(D, Z)\n",
    "print(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the least-squares problem for the operator matrix.\n",
    "Ohat = solver.solve()\n",
    "print(f\"{Ohat.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how well the solution matches the data.\n",
    "print(\"\\nOptimization residuals:\", solver.residual(Ohat), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is often advantageous to add a *regularization term* $\\mathcal{R}(\\Ohat)$ to penalize the entries of the inferred operators.\n",
    "This can prevent over-fitting to data and promote stability in the learned reduced-order model {cite}`mcquarrie2021combustion`.\n",
    "The least-squares regression {eq}`eq:lstsq:plain` then becomes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\Ohat}\\|\n",
    "        \\D\\Ohat\\trp - \\Z\\trp\n",
    "    \\|_{F}^{2} + \\mathcal{R}(\\Ohat).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "A [Tikhonov regularization](https://en.wikipedia.org/wiki/Ridge_regression#Tikhonov_regularization) term has the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathcal{R}(\\Ohat)\n",
    "    = \\sum_{i=1}^{r}\\|\\bfGamma_i\\ohat_i\\|_2^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\ohat_1,\\ldots,\\ohat_r$ are the rows of $\\Ohat$ and each $\\bfGamma_1,\\ldots,\\bfGamma_r$ is a $d \\times d$ symmetric positive-definite matrix.\n",
    "In this case, the decoupled regressions for the rows of $\\Ohat$ are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\ohat_i} \\|\\D\\ohat_i - \\z_i\\|_2^2 + \\|\\bfGamma_i\\ohat_i\\|_2^2,\n",
    "    \\quad i = 1, \\ldots, r.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The following classes solve Tikhonov-regularized least-squares Operator Inference regressions for different choices of the regularization term $\\mathcal{R}(\\Ohat)$.\n",
    "\n",
    "| Solver class                     | Description                                      | Regularization $\\mathcal{R}(\\Ohat)$            |\n",
    "| :------------------------------- | :----------------------------------------------- | :--------------------------------------------: |\n",
    "| {class}`L2Solver`                | One scalar regularizer for all $\\ohat_i$         | $\\lambda^{2}\\|\\|\\Ohat\\trp\\|\\|_F^2$             |\n",
    "| {class}`L2DecoupledSolver`       | Different scalar regularizers for each $\\ohat_i$ | $\\sum_{i=1}^{r}\\lambda_i^2\\|\\|\\ohat_i\\|\\|_2^2$ |\n",
    "| {class}`TikhonovSolver`          | One matrix regularizer for all $\\ohat_i$         | $\\|\\|\\bfGamma\\Ohat\\trp\\|\\|_F^2$                |\n",
    "| {class}`TikhonovDecoupledSolver` | Different matrix regularizers for each $\\ohat_i$ | $\\sum_{i=1}^{r}\\|\\|\\bfGamma_i\\ohat_i\\|\\|_2^2$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single scalar regularizer lambda=1e-6.\n",
    "l2solver = opinf.lstsq.L2Solver(regularizer=1e-6).fit(D, Z)\n",
    "print(l2solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_L2 = l2solver.solve()\n",
    "l2solver.residual(Ohat_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different scalar regularizer for each row of the operator matrix.\n",
    "lambdas = np.logspace(-10, r - 11, r)\n",
    "l2dsolver = opinf.lstsq.L2DecoupledSolver(regularizer=lambdas)\n",
    "l2dsolver.fit(D, Z)\n",
    "print(l2dsolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_L2d = l2dsolver.solve()\n",
    "l2dsolver.residual(Ohat_L2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single diagonal matrix regularizer.\n",
    "gammadiag = np.full(d, 1e-8)\n",
    "gammadiag[-1] = 1e-4\n",
    "tiksolver = opinf.lstsq.TikhonovSolver(regularizer=gammadiag)\n",
    "tiksolver.fit(D, Z)\n",
    "print(tiksolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_tik = tiksolver.solve()\n",
    "tiksolver.residual(Ohat_tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single non-diagonal matrix regularizer.\n",
    "offdiag = 0.1 * gammadiag[:-1]\n",
    "Gamma = np.diag(gammadiag) + np.diag(offdiag, k=1) + np.diag(offdiag, k=-1)\n",
    "tiksolver.regularizer = Gamma\n",
    "print(tiksolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_tik = tiksolver.solve()\n",
    "tiksolver.residual(Ohat_tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different matrix regularizer for each row of the operator matrix.\n",
    "diags = np.exp(np.random.random((r, d)) - 10)\n",
    "tikdsolver = opinf.lstsq.TikhonovDecoupledSolver(regularizer=diags)\n",
    "tikdsolver.fit(D, Z)\n",
    "print(tikdsolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {class}`TruncatedSVDSolver` class approximates the solution to the ordinary least-squares problem {eq}`eq:lstsq:plain` by solving the related problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\argmin_{\\Ohat}\\|\\tilde{\\D}\\Ohat\\trp - \\Z\\trp\\|_{F}^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\D}$ is the best rank-$d'$ approximation of $\\D$ for some given $d' < \\min(k,d)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{D}\n",
    "    = \\argmin_{\\D' \\in \\RR^{k \\times d}}\n",
    "    \\|\\D' - \\D\\|_{F}\n",
    "    \\quad\\textrm{such that}\\quad\n",
    "    \\operatorname{rank}(\\D') = d'.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This approach is [related to Tikhonov regularization](https://math.stackexchange.com/questions/1084677/tikhonov-regularization-vs-truncated-svd) and is based on the [truncated singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD) of the data matrix $\\D$.\n",
    "The optimization residual is guaranteed to be higher than when using the full SVD as in {class}`PlainSolver`, but the condition number of the truncated SVD system is lower than that of the original system.\n",
    "Truncation can play a similar role to regularization, but the hyperparameter here (the number of columns to use) is an integer, whereas the regularization hyperparameter $\\lambda$ for {class}`L2Solver` may be any positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvdsolver = opinf.lstsq.TruncatedSVDSolver(-2)\n",
    "tsvdsolver.fit(D, Z)\n",
    "print(tsvdsolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat = tsvdsolver.solve()\n",
    "tsvdsolver.residual(Ohat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of columns used without recomputing the SVD.\n",
    "tsvdsolver.num_svdmodes = 8\n",
    "print(tsvdsolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvdsolver.residual(tsvdsolver.solve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Least-Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear least-squares models for $\\D\\Ohat\\trp \\approx \\Z\\trp$ assume error in $\\Z$ only, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\D\\Ohat\\trp = \\Z\\trp + \\Delta_{\\Z\\trp}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for some $\\Delta_{\\Z\\trp} \\in \\RR^{r\\times k}$\n",
    "[Total least-squares](https://en.wikipedia.org/wiki/Total_least_squares) is an alternative approach that assumes possible error in the data matrix $\\D$ as well as in $\\Z$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    (\\D + \\Delta_{\\D})\\Ohat\\trp = \\Z\\trp + \\Delta_{\\Z\\trp}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for some $\\Delta_{\\D}\\in\\RR^{k \\times d}$ and $\\Delta_{\\Z\\trp}\\in\\RR^{r \\times k}$.\n",
    "\n",
    "The {class}`TotalLeastSquaresSolver` class performs a total least-squares solve for $\\Ohat$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsolver = opinf.lstsq.TotalLeastSquaresSolver()\n",
    "totalsolver.fit(D, Z)\n",
    "print(totalsolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_total = totalsolver.solve()\n",
    "totalsolver.residual(Ohat_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New solvers can be defined by inheriting from {class}`SolverTemplate`.\n",
    "Once implemented, the [`verify()`](SolverTemplate.verify) method may be used to test for consistency between the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySolver(opinf.lstsq.SolverTemplate):\n",
    "    \"\"\"Custom solver for the Operator Inference regression.\"\"\"\n",
    "\n",
    "    # Constructor -------------------------------------------------------------\n",
    "    def __init__(self, hyperparameters):\n",
    "        \"\"\"Set any solver hyperparameters.\n",
    "        If there are no hyperparameters, __init__() may be omitted.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Process / store hyperparameters here.\n",
    "\n",
    "    # Required methods --------------------------------------------------------\n",
    "    def solve(self):\n",
    "        \"\"\"Solve the regression and return the operator matrix.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Optional methods --------------------------------------------------------\n",
    "    # These may be deleted if not implemented.\n",
    "    def fit(self, data_matrix, lhs_matrix):\n",
    "        \"\"\"Save data matrices and prepare to solve the regression problem.\n",
    "        This method should do as much work as possible that does not depend on\n",
    "        the hyperparameters in preparation for solving the regression.\n",
    "        If there are no hyperparameters, fit() may be omitted.\n",
    "        \"\"\"\n",
    "        super().fit(data_matrix, lhs_matrix)\n",
    "        # Prepare for regression here.\n",
    "\n",
    "    def save(self, savefile, overwrite=False):\n",
    "        \"\"\"Save the solver to an HDF5 file.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, loadfile):\n",
    "        \"\"\"Load a solver from an HDF5 file.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Make a copy of the solver.\n",
    "        If not implemented, copy.deepcopy() is used.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See {class}`SolverTemplate` for solver attributes and details on the arguments for each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Wrapping a Least-squares Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class shows how to wrap an existing numerical least-squares solver routine.\n",
    "Here, we use `scipy.optimize.nnls()`, which solves the least-squares problem with non-negative constraints, i.e., the entries of $\\Ohat$ will all be positive.\n",
    "**This is just a demonstration** -- the entries of a good $\\Ohat$ are rarely all positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNSolver(opinf.lstsq.SolverTemplate):\n",
    "    \"\"\"Least-squares solver with non-negativity constraints.\"\"\"\n",
    "\n",
    "    def __init__(self, maxiter=None, atol=None):\n",
    "        \"\"\"Save keyword arguments for scipy.optimize.nnls().\"\"\"\n",
    "        super().__init__()\n",
    "        self.options = dict(maxiter=maxiter, atol=atol)\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"Solve the regression and return the operator matrix.\"\"\"\n",
    "        # Allocate space for the operator matrix entries.\n",
    "        Ohat = np.empty((self.r, self.d))\n",
    "\n",
    "        # Solve the nonnegative least-squares for each operator matrix row.\n",
    "        for i in range(self.r):\n",
    "            Ohat[i] = opt.nnls(\n",
    "                self.data_matrix, self.lhs_matrix[i], **self.options\n",
    "            )[0]\n",
    "\n",
    "        return Ohat\n",
    "\n",
    "        # Alternative implementation:\n",
    "        return np.array(\n",
    "            [\n",
    "                opt.nnls(self.data_matrix, z_i, **self.options)[0]\n",
    "                for z_i in self.lhs_matrix\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = NNSolver().fit(D, Z)\n",
    "print(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.verify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ohat_nn = solver.solve()\n",
    "print(f\"{Ohat_nn.shape=}\")\n",
    "\n",
    "# Check that the entries of the operator matrix are nonnegative.\n",
    "print(\"Minimal entry of Ohat_nn:\", Ohat_nn.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
