{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Heat Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "The fundamental goal of model reduction is to efficiently make physics-based predictions. Given synthetic or experimental data that was generated or collected under a certain set of conditions, we aim to construct a cost-effective model that produces accurate solutions under new sets of conditions. This tutorial explores the following prediction problems for the heat equation example of {cite}`peherstorfer2016opinf`:\n",
    "1. Predicting **forward in time**.\n",
    "2. Using new time-dependent **boundary conditions**.\n",
    "3. Changing the **system parameters** (e.g., coefficients in the governing equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-nb-collapsed": true
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Governing Equations\n",
    ":class: attention\n",
    "\n",
    "Let $\\Omega = [0,L]\\subset \\mathbb{R}$ be the spatial domain indicated by the variable $x$, and let $[0,T]\\subset\\mathbb{R}$ be the time domain with variable $t$. We consider the one-dimensional heat equation with non-homogeneous Dirichlet boundary conditions,\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\frac{\\partial}{\\partial t} q(x,t;\\mu) = \\mu\\frac{\\partial^2}{\\partial x^2}q(x,t;\\mu)\n",
    "    & x &\\in\\Omega,\\quad t\\in[0,T],\n",
    "    \\\\\n",
    "    &q(0,t;\\mu) = q(L,t;\\mu) = u(t)\n",
    "    & t &\\in[0,T],\n",
    "    \\\\\n",
    "    &q(x,0;\\mu) = \\big(e^{\\alpha(x - 1)} + e^{-\\alpha x} - e^{-\\alpha}\\big)u(0)\n",
    "    & x &\\in \\Omega,\n",
    "\\end{align*}\n",
    "\n",
    "where the constant $\\mu > 0$ is the thermal diffusivity, $\\alpha>0$ is constant, and $q(x,t;\\mu)$ is the unknown state variable. This is a model for a one-dimensional rod conducting heat with a fixed initial heat profile. The temperature at the ends of the rod are governed by the input function $u(t)$, but  heat is allowed to diffuse through the rod and flow out at the ends of the domain. We aim to numerically solve for $q(x,t;\\mu)$ efficiently for all $t \\in [0,T]$ and/or for various choices of $u(t)$ and $\\mu$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "This problem can be solved with a straightforward discretization of the spatial domain $\\Omega$ with little computational effort, so using model reduction to speed up the computation is not highly beneficial. However, the way that the user interacts with the package for this problem is highly similar for more complex problems.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction in Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first objective is to get solutions in time beyond a set of available training data.\n",
    "\n",
    ":::{image} ../../images/summary.svg\n",
    ":align: center\n",
    ":width: 80 %\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Objective\n",
    ":class: attention\n",
    "\n",
    "Construct a reduced-order model (ROM) of the heat equation that is **predictive in time**. In other words, we will observe data for $t \\in [0, T']$ with $T' < T$, use that data to construct the ROM, and use the ROM to predict the solution for the entire time domain $[0,T]$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-order Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the last tutorial, we use a centered finite difference approximation for the spatial derivative to arrive at a first-order system, this time of the form\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t;\\mu)\n",
    "    = \\mathbf{A}(\\mu)\\mathbf{q}(t;\\mu) + \\mathbf{B}(\\mu)u(t),\n",
    "    \\qquad\n",
    "    \\mathbf{q}(0;\\mu)\n",
    "    = \\mathbf{q}_0.\n",
    "$$ (eq_heat_fom_parametric)\n",
    "\n",
    ":::{dropdown} Discretization details\n",
    "\n",
    "We take an equidistant grid $\\{x_i\\}_{i=0}^{n+1} \\subset \\Omega$,\n",
    "\n",
    "\\begin{align*}\n",
    "    0 &= x_0 < x_1 < \\cdots < x_n < x_{n+1} = L\n",
    "    &\n",
    "    &\\text{and}\n",
    "    &\n",
    "    \\delta x &= \\frac{L}{n+1} = x_{i+1} - x_{i},\\quad i=1,\\ldots,n-1.\n",
    "\\end{align*}\n",
    "\n",
    "The boundary conditions prescribe $q(x_0,t) = q(x_{n+1},t) = u(t)$. Our goal is to compute $q(x,t)$ at the interior spatial points $x_{1},x_{2},\\ldots,x_{n}$ for various $t\\in[0,T]$, so we consider the state vector $\\mathbf{q}(t) = [~q(x_{1}, t)~\\cdots~q(x_{n}, t)~]^{\\top}\\in\\mathbb{R}^n$ and derive a system governing the evolution of $\\mathbf{q}(t)$ in time.\n",
    "\n",
    "Approximating the spatial derivative with a central finite difference approximation,\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial^2}{\\partial x^2}q(x,t)\n",
    "    \\approx \\frac{q(x-\\delta x,t) - 2q(x,t) + q(x+\\delta x,t)}{(\\delta x)^2},\n",
    "$$\n",
    "\n",
    "we arrive at the following matrices for the full-order model.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{A}(\\mu) &= \\frac{\\mu}{(\\delta x)^2}\\left[\\begin{array}{ccccc}\n",
    "        -2 & 1 & & & \\\\\n",
    "        1 & -2 & 1 & & \\\\\n",
    "        & \\ddots & \\ddots & \\ddots & \\\\\n",
    "        & & 1 & -2 & 1 \\\\\n",
    "        & & & 1 & -2 \\\\\n",
    "    \\end{array}\\right] \\in\\mathbb{R}^{n\\times n},\n",
    "    &\n",
    "    \\mathbf{B}(\\mu) &= \\frac{\\mu}{(\\delta x)^2}\\left[\\begin{array}{c}\n",
    "        1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1\n",
    "    \\end{array}\\right]\\in\\mathbb{R}^{n}.\n",
    "\\end{align*}\n",
    ":::\n",
    "\n",
    "The state $\\mathbf{q}(t;\\mu)$ implicity depends on the parameter $\\mu$ because the operators $\\mathbf{A}(\\mu)$ and $\\mathbf{B}(\\mu)$ are parameterized by $\\mu$.\n",
    "For now, we set $\\mu = 1$ and simply write\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t)\n",
    "    = \\mathbf{A}\\mathbf{q}(t) + \\mathbf{B}u(t),\n",
    "    \\qquad\n",
    "    \\mathbf{q}(0)\n",
    "    = \\mathbf{q}_0.\n",
    "$$\n",
    "\n",
    "This is the _full-order model_ (FOM), which we will use to generate training data for the time domain $[0, T'] \\subset [0, T]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $L = T = \\mu = 1$, $\\alpha = 100$, and suppose for now that the boundary conditions are given by the constant input function $u(t) \\equiv 1$.\n",
    "We begin by simulating the full-order system described above with a uniform time step $\\delta t = 10^{-3}$, yielding $10^3 + 1 = 1001$ total time steps (1000 steps past the initial condition).\n",
    "We will assume that we can only observe the first $k = 100$ time steps and use the ROM to predict the remaining $901$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as la\n",
    "import scipy.sparse as sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Matplotlib customizations.\n",
    "plt.rc(\"axes.spines\", right=False, top=False)\n",
    "plt.rc(\"figure\", dpi=300, figsize=(9, 3))\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "plt.rc(\"legend\", edgecolor=\"none\", frameon=True)\n",
    "plt.rc(\"text\", usetex=True)\n",
    "\n",
    "# Pandas display options.\n",
    "pd.options.display.float_format = \"{:.4%}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the spatial domain.\n",
    "L = 1                           # Spatial domain length.\n",
    "n = 2**7 - 1                    # Spatial grid size.\n",
    "x_all = np.linspace(0, L, n+2)  # Full spatial grid.\n",
    "x = x_all[1:-1]                 # Interior spatial grid (where q is unknown).\n",
    "dx = x[1] - x[0]                # Spatial resolution.\n",
    "\n",
    "# Construct the temporal domain.\n",
    "T = 1                           # Temporal domain length (final simulation time).\n",
    "K = T*10**3 + 1                 # Temporal grid size.\n",
    "t = np.linspace(0, T, K)        # Temporal grid.\n",
    "dt = t[1] - t[0]                # Temporal resolution.\n",
    "\n",
    "print(f\"Spatial step size δx = {dx}\")\n",
    "print(f\"Temporal step size δt = {dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Construct the full-order state matrix A.\n",
    "dx2inv = 1 / dx**2\n",
    "diags = np.array([1, -2, 1]) * dx2inv\n",
    "A = sparse.diags(diags, [-1, 0, 1], (n, n))\n",
    "\n",
    "# Construct the full-order input matrix B.\n",
    "B = np.zeros_like(x)\n",
    "B[0], B[-1] = dx2inv, dx2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs.\n",
    "input_func = np.ones_like       # Constant input function u(t) = 1.\n",
    "U_all = input_func(t)           # Inputs over the time domain.\n",
    "\n",
    "# Construct the initial condition.\n",
    "alpha = 100\n",
    "q0 = np.exp(alpha*(x-1)) + np.exp(-alpha*x) - np.exp(-alpha)\n",
    "\n",
    "print(f\"shape of A:\\t{A.shape}\")\n",
    "print(f\"shape of B:\\t{B.shape}\")\n",
    "print(f\"shape of q0:\\t{q0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a diffusive problem, we will use the implicit (backward) Euler method for solving the ODEs.\n",
    "For the problem $\\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t) = \\mathbf{F}(t, \\mathbf{q}(t), \\mathbf{u}(t))$, implicit Euler is defined by the rule\n",
    "\n",
    "$$\n",
    "    \\mathbf{q}_{j+1} = \\mathbf{q}_{j} + \\delta t \\mathbf{F}(t_{j+1},\\mathbf{q}_{j+1},u_{j+1}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{q}_{j} := \\mathbf{q}(t_{j})$ and $u_{j} := u(t_{j})$.\n",
    "With the form $\\mathbf{F}(t,\\mathbf{q}(t),u(t)) = \\mathbf{A}\\mathbf{q}(t) + \\mathbf{B}u(t)$, this becomes\n",
    "\n",
    "$$\n",
    "    \\mathbf{q}_{j+1} = (\\mathbf{I} - \\delta t \\mathbf{A})^{-1}\\left(\\mathbf{q}_{j} + \\delta t \\mathbf{B} u_{j+1}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def implicit_euler(t, q0, A, B, U):\n",
    "    \"\"\"Solve the system\n",
    "\n",
    "        dq / dt = Aq(t) + Bu(t),    q(0) = q0,\n",
    "\n",
    "    over a uniform time domain via the implicit Euler method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : (k,) ndarray\n",
    "        Uniform time array over which to solve the ODE.\n",
    "    q0 : (n,) ndarray\n",
    "        Initial condition.\n",
    "    A : (n, n) ndarray\n",
    "        State matrix.\n",
    "    B : (n,) or (n, 1) ndarray\n",
    "        Input matrix.\n",
    "    U : (k,) ndarray\n",
    "        Inputs over the time array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : (n, k) ndarray\n",
    "        Solution to the ODE at time t; that is, q[:,j] is the\n",
    "        computed solution corresponding to time t[j].\n",
    "    \"\"\"\n",
    "    # Check and store dimensions.\n",
    "    k = len(t)\n",
    "    n = len(q0)\n",
    "    B = np.ravel(B)\n",
    "    assert A.shape == (n, n)\n",
    "    assert B.shape == (n,)\n",
    "    assert U.shape == (k,)\n",
    "    I = np.eye(n)\n",
    "\n",
    "    # Check that the time step is uniform.\n",
    "    dt = t[1] - t[0]\n",
    "    assert np.allclose(np.diff(t), dt)\n",
    "\n",
    "    # Factor I - dt*A for quick solving at each time step.\n",
    "    factored = la.lu_factor(I - dt*A)\n",
    "\n",
    "    # Solve the problem by stepping in time.\n",
    "    q = np.empty((n, k))\n",
    "    q[:,0] = q0.copy()\n",
    "    for j in range(1, k):\n",
    "        q[:, j] = la.lu_solve(factored, q[:, j-1] + dt*B*U[j])\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute snapshots by solving the equation with implicit_euler().\n",
    "Q_all = implicit_euler(t, q0, A, B, U_all)\n",
    "\n",
    "# Retain only the first k snapshots/inputs for training the ROM.\n",
    "k = 100                         # Number of training snapshots.\n",
    "t_train = t[:k]                 # Temporal domain for training snapshots.\n",
    "Q = Q_all[:, :k]                # Observed snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the time derivatives $\\frac{d\\mathbf{q}}{dt}$ of the training snapshots. If $\\mathbf{A}$ and $\\mathbf{B}$ are known, we can set $\\dot{\\mathbf{q}}_{j} = \\mathbf{A}\\mathbf{q}_{j} + \\mathbf{B}u_{j}$. If we do not have access to $\\mathbf{A}$ and $\\mathbf{B}$, we can estimate the time derivatives using finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate time derivatives (dq/dt) for each training snapshot.\n",
    "Qdot_train = (Q[:, 1:] - Q[:, :-1]) / dt\n",
    "Q_train = Q[:, 1:]              # Training snapshots.\n",
    "U_train = U_all[1:k]            # Training inputs.\n",
    "\n",
    "print(f\"shape of Q_train:\\t{Q_train.shape}\")\n",
    "print(f\"shape of Qdot_train:\\t{Qdot_train.shape}\")\n",
    "print(f\"shape of U_train:\\t{U_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the snapshots to get a sense of how the solution looks qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_heat_data(Z, title, ax=None):\n",
    "    \"\"\"Visualize temperature data in space and time.\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # Plot a few snapshots over the spatial domain.\n",
    "    sample_columns = [0, 10, 20, 40, 80, 160, 320, 640]\n",
    "    sample_columns = [0] + [2**d for d in range(10)]\n",
    "    color = iter(plt.cm.viridis_r(np.linspace(.05, 1, len(sample_columns))))\n",
    "    while sample_columns[-1] > Z.shape[1]:\n",
    "        sample_columns.pop()\n",
    "    leftBC, rightBC = [input_func(x_all[0])], [input_func(x_all[-1])]\n",
    "    for j in sample_columns:\n",
    "        q_all = np.concatenate([leftBC, Z[:,j], rightBC])\n",
    "        ax.plot(x_all, q_all, color=next(color), label=fr\"$q(x,t_{{{j}}})$\")\n",
    "\n",
    "    ax.set_xlim(x_all[0], x_all[-1])\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$q(x,t)$\")\n",
    "    ax.legend(loc=(1.05, .05))\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2)\n",
    "plot_heat_data(Q, \"Snapshot data for training\", ax1)\n",
    "plot_heat_data(Q_all, \"Full-order model solution\", ax2)\n",
    "ax1.legend([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have snapshot data $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$, we can construct [a basis matrix](sec-basis-computation) $\\mathbf{V}_r \\in \\mathbb{R}^{n \\times r}$. The basis matrix relates the high-dimensional and low-dimensional by $\\mathbf{q}(t) = \\mathbf{V}_{r}\\widehat{\\mathbf{q}}(t)$.\n",
    "\n",
    "For operator inference (OpInf), we often use the [proper orthogonal decomposition](subsec-pod) (POD) basis. The integer $r$, which defines the dimension of the reduced-order model to be constructed, is usually determined by how quickly the singular values of $\\mathbf{Q}$ decay. In this example, we choose the minimal $r$ such that the [residual energy](subsec-basis-size) is less than a given tolerance $\\varepsilon$, i.e.,\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{j=r + 1}^{k}\\sigma_{j}^{2}}{\\sum_{j=1}^{k}\\sigma_{j}^{2}} = \\frac{||\\mathbf{Q} - \\mathbf{V}_r \\mathbf{V}_r^{\\top}\\mathbf{Q}||_{F}^{2}}{||\\mathbf{Q}||_{F}^{2}} < \\varepsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the POD basis, using the residual energy decay to select r.\n",
    "basis = opinf.pre.PODBasis().fit(Q, residual_energy=1e-8)\n",
    "print(basis)\n",
    "\n",
    "# Check the decay of the singular values.\n",
    "basis.plot_svdval_decay()\n",
    "plt.xlim(0, 25)\n",
    "\n",
    "# Check the decay of the residual energy based on the singular values.\n",
    "basis.plot_residual_energy(threshold=1e-8)\n",
    "plt.xlim(0, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{margin}\n",
    ":::{note}\n",
    "In this case, since $u(t) \\equiv 1$ is constant, we could equivalently set `modelform=\"cA\"` to learn a ROM of the form $\\frac{\\text{d}}{\\text{d}t}\\widehat{\\mathbf{q}}(t) = \\widehat{\\mathbf{c}} + \\widehat{\\mathbf{A}}\\widehat{\\mathbf{q}}(t)$, where $\\widehat{\\mathbf{c}}$ is a constant term.\n",
    "There is no difference between the two models, i.e., $\\widehat{\\mathbf{c}} = \\widehat{\\mathbf{B}}u(t) = \\widehat{\\mathbf{B}}$, except that `modelform=\"AB\"` allows us to use different inputs for $u(t)$ later on.\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can learn the reduced model with OpInf.\n",
    "Because the full-order model is of the form $\\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t) = \\mathbf{A}\\mathbf{q}(t) + \\mathbf{B}u(t)$, we set the form of the ROM to $\\frac{\\text{d}}{\\text{d}t}\\widehat{\\mathbf{q}}(t) = \\widehat{\\mathbf{A}}\\widehat{\\mathbf{q}}(t) + \\widehat{\\mathbf{B}}u(t)$ by specifying `modelform=\"AB\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "rom = opinf.ContinuousOpInfROM(modelform=\"AB\")\n",
    "rom.fit(basis=basis, states=Q_train, ddts=Qdot_train, inputs=U_train)\n",
    "print(rom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the FOM, we integrate the learned ROM using the implicit Euler method, using the reduced-order operators $\\widehat{\\mathbf{A}}$ and $\\widehat{\\mathbf{B}}$ and the initial condition $\\widehat{\\mathbf{q}}_{0} = \\mathbf{V}^{\\mathsf{T}}\\mathbf{q}_{0}$.\n",
    "The resulting low-dimensional state vectors are decoded back to the full-dimensional space via $\\mathbf{q}(t) = \\mathbf{V}_{r}\\widehat{\\mathbf{q}}(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Express the initial condition in the coordinates of the basis.\n",
    "q0_ = basis.encode(q0)\n",
    "\n",
    "# Solve the reduced-order model using Implicit Euler.\n",
    "Q_ROM = basis.decode(implicit_euler(t, q0_, rom.A_.entries, rom.B_.entries, U_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2)\n",
    "plot_heat_data(Q_ROM, \"Reduced-order model solution\", ax1)\n",
    "plot_heat_data(Q_all, \"Full-order model solution\", ax2)\n",
    "ax1.legend([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify the accuracy of the ROM, we evaluate the ROM solution error in the Frobenius norm and compare it to the projection error,\n",
    "\n",
    "$$\n",
    "    \\text{err}_{\\text{ROM}}\n",
    "    = \\frac{||\\mathbf{Q}_{\\text{all}} - \\mathbf{Q}_{\\text{ROM}}||_F}{||\\mathbf{Q}_{\\text{all}}||_F},\n",
    "    \\qquad\n",
    "    \\text{err}_{\\text{proj}}\n",
    "    = \\frac{||\\mathbf{Q}_{\\text{all}} - \\mathbf{V}_{r}\\mathbf{V}_{r}^{\\top}\\mathbf{Q}_{\\text{all}}||_F}{||\\mathbf{Q}_{\\text{all}}||_F},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}_{\\text{all}}$ is the full-order model solution over the entire time domain and $\\mathbf{Q}_{\\text{ROM}}$ is the reduced-order model solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_froerr_projection = basis.projection_error(Q_all, relative=True)\n",
    "rel_froerr_opinf = opinf.post.frobenius_error(Q_all, Q_ROM)[1]\n",
    "\n",
    "print(\"Relative Frobenius-norm errors\", '-'*33,\n",
    "      f\"projection error:\\t{rel_froerr_projection:%}\",\n",
    "      f\"OpInf ROM error:\\t{rel_froerr_opinf:%}\",\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROM error cannot be better than the projection error, but the two are pretty close. We also compare the ROM error with the projection error as a function of time, i.e.,\n",
    "\n",
    "$$\n",
    "    \\text{err}_{\\text{ROM}}(t)\n",
    "    = \\frac{\\|\\mathbf{q}(t) - \\mathbf{q}_{\\text{ROM}}(t)\\|_{2}}{\\|\\mathbf{q}(t)\\|_{2}},\n",
    "    \\qquad\n",
    "    \\text{err}_{\\text{proj}}(t)\n",
    "    = \\frac{\\|\\mathbf{q}(t) - \\mathbf{V}_{r}\\mathbf{V}_{r}^{\\mathsf{T}}\\mathbf{q}(t)\\|_{2}}{\\|\\mathbf{q}(t)\\|_{2}},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{q}(t)$ is the full-order solution and $\\mathbf{q}_{\\text{ROM}}(t)$ is the ROM solution at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "In this problem, $\\mathbf{q}(t) \\to \\mathbf{0}$ as $t$ increases, so a relative error may not be appropriate since $\\|\\mathbf{q}(t)\\|_{2}$ appears in the denominator.\n",
    "In situations like this, consider using the _normalized absolute error_ by replacing the denominator with $\\max_{\\tau\\in[0,T]}\\|\\mathbf{q}(t)\\|$, for example:\n",
    "$$\n",
    "    \\text{err}_{\\text{ROM}}(t)\n",
    "    = \\frac{\\|\\mathbf{q}(t) - \\mathbf{q}_{\\text{ROM}}(t)\\|_{2}}{\\max_{\\tau\\in[0,T]}\\|\\mathbf{q}(\\tau)\\|_{2}}.\n",
    "$$\n",
    "\n",
    "Use `normalize=True` in `opinf.post.lp_error()` to use this error measure instead of the relative error.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projerr_in_time = opinf.post.lp_error(Q_all,\n",
    "                                      basis.project(Q_all),\n",
    "                                      normalize=True)[1]\n",
    "\n",
    "def plot_errors_over_time(Zlist, labels):\n",
    "    \"\"\"Plot normalized absolute projection error and ROM errors\n",
    "    as a function of time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Zlist : list((n, k) ndarrays)\n",
    "        List of reduced-order model solutions.\n",
    "    labels : list(str)\n",
    "        Labels for each of the reduced-order models.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    ax.semilogy(t, projerr_in_time, \"C3\", label=\"Projection Error\")\n",
    "    colors = [\"C0\", \"C5\"]\n",
    "    for Z, label, c in zip(Zlist, labels, colors[:len(Zlist)]):\n",
    "        rel_err = opinf.post.lp_error(Q_all, Z, normalize=True)[1]\n",
    "        plt.semilogy(t, rel_err, c, label=label)\n",
    "\n",
    "    ax.set_xlim(t[0], t[-1])\n",
    "    ax.set_xlabel(r\"$t$\")\n",
    "    ax.set_ylabel(\"Normalized absolute error\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors_over_time([Q_ROM], [\"ROM Error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Intrusive Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the limit as the amount of training data $k$ and the dimension $r$ increases, the reduced operators $\\widehat{\\mathbf{A}}$ and $\\widehat{\\mathbf{B}}$ learned through OpInf converge to the corresponding operators obtained through _intrusive projection_,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\widetilde{\\mathbf{A}} &= \\mathbf{V}_{r}^{\\mathsf{T}} \\mathbf{A} \\mathbf{V}_{r},\n",
    "    &\n",
    "    \\widetilde{\\mathbf{B}} &= \\mathbf{V}_{r}^{\\mathsf{T}}\\mathbf{B}.\n",
    "\\end{align*}\n",
    "\n",
    "Computing $\\widetilde{\\mathbf{A}}$ and $\\widetilde{\\mathbf{B}}$ is considered \"intrusive\" because it requires explicit access to the full-order operators $\\mathbf{A}$ and $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vr = basis.entries\n",
    "Atilde = Vr.T @ A @ Vr\n",
    "Btilde = Vr.T @ B\n",
    "q0_ = basis.encode(q0)\n",
    "Q_ROM_intrusive = basis.decode(implicit_euler(t, q0_, Atilde, Btilde, U_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "The OpInf ROM classes, such as `ContinuousOpInfROM`, learn reduced-order model operators intrusively whenever the full-order operators are provided through the `known_operators` argument of `fit()`.\n",
    "This argument should be a dictionary mapping the modelform key to the full-order operator.\n",
    "If every full-order operator is provided, then the `states` and `ddts` arguments of `fit()` may be set to `None`, since there is no data-driven learning in this case.\n",
    "\n",
    "```python\n",
    ">>> rom_intrusive = opinf.ContinuousOpInfROM(\"AB\")\n",
    ">>> rom_intrusive.fit(basis, None, None, known_operators={\"A\": A, \"B\": B})\n",
    ">>> np.all(rom_intrusive.A_.entries == Atilde)\n",
    "True\n",
    ">>> np.all(rom_intrusive.B_.entries[:, 0] == Btilde)\n",
    "True\n",
    "\n",
    ">>> Q_ROM_intrusive = basis.decode(implicit_euler(t, q0_, rom_intrusive.A_.entries, rom_intrusive.B_.entries, U_all))\n",
    "```\n",
    "\n",
    "If, for example, only the full-order operator $\\mathbf{B}$ were known (but not $\\mathbf{A}$), we can learn $\\widehat{\\mathbf{A}}$ through OpInf and $\\widehat{\\mathbf{B}}$ through intrusive projection:\n",
    "\n",
    "```python\n",
    ">>> rom_partially_intrusive = opinf.ContinuousOpInfROM(\"AB\")\n",
    ">>> rom_partially_intrusive.fit(basis, Q, Qdot, U, known_operators={\"B\": B})\n",
    ">>> np.all(rom_partially_intrusive.A_.entries == Atilde)\n",
    "False\n",
    ">>> np.all(rom_partially_intrusive.B_.entries[:, 0] == Btilde)\n",
    "True\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2)\n",
    "plot_heat_data(Q_ROM, \"OpInf ROM solution\", ax1)\n",
    "plot_heat_data(Q_ROM_intrusive, \"Intrusive ROM solution\", ax2)\n",
    "ax1.legend([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_froerr_intrusive = opinf.post.frobenius_error(Q_all, Q_ROM_intrusive)[1]\n",
    "\n",
    "print(\"Relative Frobenius-norm errors\", '-'*33,\n",
    "      f\"projection error:\\t{rel_froerr_projection:%}\",\n",
    "      f\"OpInf ROM error:\\t{rel_froerr_opinf:%}\",\n",
    "      f\"intrusive ROM error:\\t{rel_froerr_intrusive:%}\",\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors_over_time([Q_ROM, Q_ROM_intrusive],\n",
    "                      [\"OpInf ROM Error\", \"Intrusive ROM Error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the experiment with different choices of $r$ to see how the size of the ROM affects its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(r):\n",
    "    \"\"\"Do OpInf / intrusive ROM prediction with r basis vectors.\"\"\"\n",
    "    basis.r = r\n",
    "    q0_ = basis.encode(q0)\n",
    "\n",
    "    # Construct and simulate the intrusive ROM.\n",
    "    rom_intrusive = opinf.ContinuousOpInfROM(\"AB\").fit(basis, None, None,\n",
    "                                                       known_operators={\"A\":A, \"B\":B})\n",
    "    Q_ROM_intrusive = basis.decode(implicit_euler(t, q0_,\n",
    "                                                  rom_intrusive.A_.entries,\n",
    "                                                  rom_intrusive.B_.entries, U_all))\n",
    "\n",
    "    # Construct and simulate the operator inference ROM.\n",
    "    rom_opinf = opinf.ContinuousOpInfROM(\"AB\").fit(basis, Q_train, Qdot_train, U_train)\n",
    "    Q_ROM_opinf = basis.decode(implicit_euler(t, q0_,\n",
    "                                              rom_opinf.A_.entries,\n",
    "                                              rom_opinf.B_.entries, U_all))\n",
    "\n",
    "    # Calculate errors.\n",
    "    projection_error = basis.projection_error(Q_all, relative=True)\n",
    "    intrusive_error = opinf.post.frobenius_error(Q_all, Q_ROM_intrusive)[1]\n",
    "    opinf_error = opinf.post.frobenius_error(Q_all, Q_ROM_opinf)[1]\n",
    "\n",
    "    return projection_error, intrusive_error, opinf_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_state_error(rmax, runner, ylabel):\n",
    "    \"\"\"Run the experiment for r = 1, ..., rmax and plot results.\"\"\"\n",
    "    rs = np.arange(1, rmax+1)\n",
    "    err_projection, err_intrusive, err_opinf = zip(*[runner(r) for r in rs])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.semilogy(rs, err_projection, 'C3-',\n",
    "                label=\"projection error\", lw=1)\n",
    "    ax.semilogy(rs, err_intrusive, 'C5+-',\n",
    "                label=\"intrusive ROM error\", lw=1, mew=2)\n",
    "    ax.semilogy(rs, err_opinf, 'C0o-',\n",
    "                label=\"OpInf ROM error\", lw=1, mfc=\"none\", mec=\"C0\", mew=1.5)\n",
    "\n",
    "    ax.set_xlim(rs.min(), rs.max())\n",
    "    ax.set_xticks(rs, [str(int(r)) for r in rs])\n",
    "    ax.set_xlabel(r\"Reduced dimension $r$\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc=\"upper right\", fontsize=14, framealpha=1)\n",
    "    ax.grid(ls=':')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_error(15, run_trial, \"Relative Frobenius-norm error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Takeaway\n",
    ":class: attention\n",
    "In this case, the operator inference and intrusive ROMs give essentially the same result.\n",
    "However, the operator inference ROM successfully emulates the FOM **without explicit access to** $\\mathbf{A}$ **and** $\\mathbf{B}$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} On Convergence\n",
    ":class: warning\n",
    "The figure above conveys a sense of convergence: as the reduced dimension $r$ increases, the ROM error decreases. In more complex problems, **the error does not always decrease monotonically as $r$ increases**. In fact, at some point as $r$ increases performance often deteriorates significantly due to poor conditioning in the operator inference regression. In practice, choose a reduced dimension $r$ that balances solution accuracy with computational speed, not too small but also not too large.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Boundary Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our heat equation has Dirichlet boundary conditions given by\n",
    "\n",
    "$$\n",
    "q(0,t;\\mu) = q(L,t;\\mu) = u(t).\n",
    "$$\n",
    "\n",
    "In this section we consider the role of $u(t)$, which governs the boundary equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Objective\n",
    ":class: attention\n",
    "\n",
    "Construct a reduced-order model (ROM) of the heat equation that can be used for various sets of boundary conditions. We will observe data for some $u(t)$ and use the ROM to predict the solution for new choices of $u(t)$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the full-order model defined in the previous section is valid for arbitrary $u(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several sets of boundary condition inputs.\n",
    "Us_all = [\n",
    "    np.ones_like(t),                        # u(t) = 1.0\n",
    "    np.exp(-t),                             # u(t) = e^(-t)\n",
    "    1 + t**2 / 2,                           # u(t) = 1 + .5 t^2\n",
    "    1 - np.sin(np.pi * t) / 2,              # u(t) = 1 - sin(πt)/2\n",
    "    1 - np.sin(3 * np.pi * t) / 3,          # u(t) = 1 - sin(3πt)/2\n",
    "    1 + 25 * (t * (t - 1))**3,              # u(t) = 1 + 25(t(t - 1))^3\n",
    "    1 + np.sin(np.pi * t) * np.exp(-2*t),   # u(t) = 1 + sin(πt)e^(-t)\n",
    "]\n",
    "\n",
    "k = 300                                     # Number of training snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $u(0) = 1$ for each of our boundary inputs, which is consistent with the initial condition `q0` used earlier.\n",
    "We will gather data for the first few inputs, learn a ROM from the data, and test the ROM on the remaining inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training / testing sets.\n",
    "Us_all_train = Us_all[:4]\n",
    "Us_all_test = Us_all[4:]\n",
    "\n",
    "# Visualize the input functions.\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2)\n",
    "c = 0\n",
    "for U in Us_all_train:\n",
    "    ax1.plot(t, U, color=f\"C{c}\")\n",
    "    c += 1\n",
    "for U in Us_all_test:\n",
    "    ax2.plot(t, U, color=f\"C{c}\")\n",
    "    c += 1\n",
    "\n",
    "ax1.set_title(\"Training inputs\")\n",
    "ax2.set_title(\"Testing inputs\")\n",
    "ax1.axvline(t[k], color='k')\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(r\"$t$\")\n",
    "    ax.set_ylabel(r\"$u(t)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only record the first $k$ snapshots corresponding to each of the training inputs, so we are still predicting in time as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute snapshots by solving the equation with implicit_euler().\n",
    "Qs_all = [implicit_euler(t, q0, A, B, U) for U in Us_all]\n",
    "Qs_all_train = Qs_all[:len(Us_all_train)]\n",
    "Qs_all_test = Qs_all[len(Us_all_train):]\n",
    "\n",
    "# Retain only the first k snapshots/inputs for training the ROM.\n",
    "t_train = t[:k]                         # Temporal domain for training snapshots.\n",
    "Qs = [Q[:, :k] for Q in Qs_all_train]   # Observed snapshots.\n",
    "\n",
    "# Compute time derivatives (dq/dt) for each snapshot and stack training data.\n",
    "Qdots_train = np.hstack([(Q[:, 1:] - Q[:, :-1]) / dt for Q in Qs])\n",
    "Qs_train = np.hstack([Q[:, 1:k] for Q in Qs])\n",
    "Us_train = np.hstack([U[1:k] for U in Us_all_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a basis from all of the training snapshots.\n",
    "basis = opinf.pre.PODBasis().fit(np.hstack(Qs), residual_energy=1e-8)\n",
    "print(basis)\n",
    "\n",
    "# Express the initial condition in the coordinates of the new basis.\n",
    "q0_ = basis.encode(q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a reduced-order model using the training data.\n",
    "rom = opinf.ContinuousOpInfROM(modelform=\"AB\")\n",
    "rom.fit(basis=basis, states=Qs_train, ddts=Qdots_train, inputs=Us_train)\n",
    "print(rom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the learned ROM on both the training and testing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_over_time_inputs(Q_ROMs, Q_trues, cidx=0):\n",
    "    \"\"\"Plot normalized absolute projection error and ROM errors\n",
    "    as a function of time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_ROMs : list((n, k) ndarrays)\n",
    "        List of reduced-order model solutions.\n",
    "    Q_trues : list(str)\n",
    "        List of full-order model solutions.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    \n",
    "    for Q_ROM, Q_true in zip(Q_ROMs, Q_trues):\n",
    "        rel_err = opinf.post.lp_error(Q_true, Q_ROM, normalize=True)[1]\n",
    "        plt.semilogy(t, rel_err, color=f\"C{cidx}\")\n",
    "        cidx += 1\n",
    "\n",
    "    ax.set_xlim(t[0], t[-1])\n",
    "    ax.set_xlabel(r\"$t$\")\n",
    "    ax.set_ylabel(\"Normalized absolute error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ROM accuracy on the training inputs.\n",
    "Qs_ROM_train = [\n",
    "    basis.decode(implicit_euler(t, q0_, rom.A_.entries, rom.B_.entries, U))\n",
    "    for U in Us_all_train\n",
    "]\n",
    "plot_errors_over_time_inputs(Qs_ROM_train, Qs_all_train)\n",
    "plt.title(\"ROM error with training inputs\")\n",
    "\n",
    "# Test ROM accuracy on the testing inputs.\n",
    "Qs_ROM_test = [\n",
    "    basis.decode(implicit_euler(t, q0_, rom.A_.entries, rom.B_.entries, U))\n",
    "    for U in Us_all_test\n",
    "]\n",
    "plot_errors_over_time_inputs(Qs_ROM_test, Qs_all_test,\n",
    "                             cidx=len(Qs_ROM_train))\n",
    "plt.title(\"ROM error with testing inputs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, the training and testing error are similar and small (less than 0.1%) throughout the time domain. We conclude this section by checking the average ROM error on the test inputs as a function of basis size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial_inputs(r):\n",
    "    \"\"\"Do OpInf / intrusive ROM prediction with r basis vectors.\"\"\"\n",
    "    basis.r = r\n",
    "    q0_ = basis.encode(q0)\n",
    "\n",
    "    # Construct the intrusive ROM.\n",
    "    rom_intrusive = opinf.ContinuousOpInfROM(\"AB\").fit(basis, None, None,\n",
    "                                                       known_operators={\"A\":A, \"B\":B})\n",
    "\n",
    "    # Construct the operator inference ROM from the training data.\n",
    "    rom_opinf = opinf.ContinuousOpInfROM(modelform=\"AB\")\n",
    "    rom_opinf.fit(basis=basis, states=Qs_train, ddts=Qdots_train, inputs=Us_train)\n",
    "\n",
    "    # Test the ROMs at each testing input.\n",
    "    projection_error, intrusive_error, opinf_error = 0, 0, 0\n",
    "    for Q, U in zip(Qs_all_test, Us_all_test):\n",
    "\n",
    "        # Simulate the intrusive ROM for this testing input.\n",
    "        Q_ROM_intrusive = basis.decode(implicit_euler(t, q0_,\n",
    "                                                      rom_intrusive.A_.entries,\n",
    "                                                      rom_intrusive.B_.entries, U))\n",
    "\n",
    "        # Simulate the operator inference ROM for this testing input.\n",
    "        Q_ROM_opinf = basis.decode(implicit_euler(t, q0_,\n",
    "                                                  rom_opinf.A_.entries,\n",
    "                                                  rom_opinf.B_.entries, U))\n",
    "\n",
    "        # Calculate errors.\n",
    "        projection_error += basis.projection_error(Q, relative=True)\n",
    "        intrusive_error += opinf.post.frobenius_error(Q, Q_ROM_intrusive)[1]\n",
    "        opinf_error += opinf.post.frobenius_error(Q, Q_ROM_opinf)[1]\n",
    "\n",
    "    # Average the relative errors.\n",
    "    projection_error /= len(Us_all_test)\n",
    "    intrusive_error /= len(Us_all_test)\n",
    "    opinf_error /= len(Us_all_test)\n",
    "\n",
    "    return projection_error, intrusive_error, opinf_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_error(15, run_trial_inputs, \"Average relative\\nFrobenius-norm error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment shows that the operator inference ROMs is robust to new boundary conditions; in other words, the ROM learns an input operator $\\widehat{\\mathbf{B}}$ that performs well for multiple choices of the input $u(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction in Parameter Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the governing equation,\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial t} q(x,t;{\\color{teal}\\mu})\n",
    "    = {\\color{teal}\\mu}\\frac{\\partial^2}{\\partial x^2}q(x,t;{\\color{teal}\\mu}).\n",
    "$$\n",
    "\n",
    "In this section we examine the role of the constant $\\mu > 0$, the heat diffusivity parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Objective\n",
    ":class: attention\n",
    "\n",
    "Construct a ROM of the heat equation that can be solved for different choices of the diffusivity parameter $\\mu > 0$.\n",
    "We will observe data for a few values of $\\mu$ and use the ROM to predict the solution for new values of $\\mu$. As before, we also aim to be predictive in time.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-order Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solved this problem earlier for fixed $\\mu = 1$.\n",
    "For variable $\\mu$, {eq}`eq_heat_fom_parametric` defines the full-order model:\n",
    "\n",
    "$$\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t;\\mu)\n",
    "    = \\mathbf{A}(\\mu)\\mathbf{q}(t;\\mu) + \\mathbf{B}(\\mu)u(t),\n",
    "    \\qquad\n",
    "    \\mathbf{q}(0;\\mu)\n",
    "    = \\mathbf{q}_0.\n",
    "$$\n",
    "\n",
    "Note that $\\mathbf{A}(\\mu) = \\mu \\mathbf{A}(1)$ and $\\mathbf{B}(\\mu) = \\mu \\mathbf{B}(1)$, and that $\\mathbf{A}(1)$ and $\\mathbf{B}(1)$ are the full-order operators we constructed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the parameter domain $\\mathcal{D} = [.1, 10] \\subset \\mathbb{R}$.\n",
    "Taking $s$ logarithmically spaced samples $\\{\\mu_i\\}_{i=1}^{s}\\subset\\mathcal{D}$, we solve the full-order model over $[0, T']$ for each parameter sample.\n",
    "For each parameter $\\mu_{i}$, the resulting snapshots matrix is denoted as $\\mathbf{Q}(\\mu_{i})\\in \\mathbb{R}^{n \\times k}$.\n",
    "We choose $s = 10$ training parameters in the following experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10                          # Number of parameter samples.\n",
    "params = np.logspace(-1, 1, s)  # Get s logarithmically spaced values of µ from D = [.1, 10].\n",
    "\n",
    "# Retain only the first k snapshots/inputs for training the ROM.\n",
    "k = 600                         # Number of training snapshots.\n",
    "t_train = t[:k]                 # Temporal domain for training snapshots.\n",
    "U_train = U_all[:k]\n",
    "\n",
    "# Solve the full-order model at each of the parameter samples.\n",
    "Qs = [implicit_euler(t_train, q0, µ * A, µ * B, U_train) for µ in params]\n",
    "Qdots_train = [(Q[:, 1:] - Q[:, :-1]) / dt for Q in Qs]\n",
    "Qs_train = [Q[:, 1:] for Q in Qs]\n",
    "Us_train = [U_train[1:] for _ in Qs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (global) POD basis can be constructed from the concatenation of the individual snapshot matrices,\n",
    "\n",
    "$$\n",
    "    \\mathbf{Q}\n",
    "    = \\left[~\\mathbf{Q}(\\mu_1)~\\cdots~\\mathbf{Q}(\\mu_s)~\\right]\n",
    "    \\in\\mathbb{R}^{n \\times sk}.\n",
    "$$\n",
    "\n",
    "We can select the reduced dimension $r$ as before by examining the residual energy of the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the POD basis, using the residual energy decay to select r.\n",
    "basis = opinf.pre.PODBasis().fit(np.hstack(Qs), residual_energy=1e-8)\n",
    "print(basis)\n",
    "\n",
    "# Check the decay of the singular values.\n",
    "basis.plot_svdval_decay()\n",
    "plt.xlim(0, 30)\n",
    "\n",
    "# Check the decay of the residual energy based on the singular values.\n",
    "basis.plot_residual_energy(threshold=1e-8)\n",
    "plt.xlim(0, 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could choose $r$ so that the average relative projection error,\n",
    "\n",
    "$$\n",
    "    \\text{avgerr}_\\text{proj} = \\frac{1}{s}\\sum_{i=1}^{s}\\frac{||\\mathbf{Q}(\\mu_i) - \\mathbf{V}_r \\mathbf{V}_r^{\\top}\\mathbf{Q}(\\mu_i)||_F}{||\\mathbf{Q}(\\mu_i)||_F},\n",
    "$$\n",
    "\n",
    "is below a certain threshold, say $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_relative_projection_error(r):\n",
    "    \"\"\"Compute the average relative projection error with r basis vectors.\"\"\"\n",
    "    oldr = basis.r\n",
    "    basis.r = r\n",
    "    avgerr = np.mean([basis.projection_error(Q, relative=True) for Q in Qs])\n",
    "    basis.r = oldr\n",
    "    return avgerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "rs = np.arange(1, 21)\n",
    "ax.axhline(1e-5, color=\"gray\", lw=1)\n",
    "ax.axvline(10, color=\"gray\", lw=1)\n",
    "ax.semilogy(rs, [average_relative_projection_error(r) for r in rs],\n",
    "            \"C3.-\", ms=10)\n",
    "ax.set_xticks(rs[::2])\n",
    "ax.set_xlabel(r\"$r$\")\n",
    "ax.set_ylabel(\"Average relative\\nprojection error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these criteria, we choose $r = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis.r = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolatory Operator Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several strategies to account for the parameter $\\mu$. The reduced-order operators obtained through intrusive projection are given by\n",
    "\n",
    "$$\n",
    "    \\widetilde{\\mathbf{A}}(\\mu)\n",
    "    = \\mathbf{V}_{r}^{\\mathsf{T}} \\mathbf{A}(\\mu) \\mathbf{V}_{r},\n",
    "    \\qquad\n",
    "    \\widetilde{\\mathbf{B}}(\\mu)\n",
    "    = \\mathbf{V}_{r}^{\\mathsf{T}} \\mathbf{B}(\\mu).\n",
    "$$\n",
    "\n",
    "Here, we perform interpolation on the entries of the reduced-order operators learned for each parameter sample. This means we learn a separate ROM for each $\\mu_i$, $i=1, \\ldots, s$, obtaining reduced-order operators $\\widehat{\\mathbf{A}}(\\mu_{i})$ and $\\widehat{\\mathbf{B}}(\\mu_{i})$.\n",
    "Then, for a new parameter value $\\bar{\\mu}\\in\\mathcal{D}$, we interpolate the entries of the learned reduced model operators to create a new reduced model corresponding to $\\bar{\\mu}\\in\\mathcal{D}$.\n",
    "<!-- \n",
    "$$\n",
    "    \\widehat{\\mathbf{A}}(\\bar{\\mu})_{i,j}\n",
    "    = \\text{interpolate}\\left(\\mathbf{A}(\\mu_{1})_{i,j}, \\ldots, \\mathbf{A}(\\mu_{s})_{i,j}; \\bar{\\mu}\\right)\n",
    "$$\n",
    " -->\n",
    "The `InterpolatedContinuousOpInfROM` class encapsulates this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn reduced models for each parameter value.\n",
    "rom = opinf.InterpolatedContinuousOpInfROM(\"AB\")\n",
    "rom.fit(basis=basis, parameters=params, states=Qs_train, ddts=Qdots_train, inputs=Us_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{tip}\n",
    "If we can construct $\\mathbf{A}(\\mu_{i})$ and $\\mathbf{B}(\\mu_{i})$ for each parameter sample, then we can learn an intrusive interpolatory ROM with `InterpolatedContinuousOpInfROM` through the `known_operators` argument.\n",
    "\n",
    "```python\n",
    ">>> rom_intrusive = opinf.InterpolatedContinuousOpInfROM(\"AB\")\n",
    ">>> rom_intrusive.fit(basis, params, None, None,\n",
    "                      known_operators={\"A\": [µ * A for µ in params],\n",
    "                                       \"B\": [µ * B for µ in params]})\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the ROM, we take $s$ logarithmically distributed random samples in $\\mathcal{D}$ and compute the average relative state error,\n",
    "\n",
    "$$\n",
    "    \\text{avgerr}_\\text{ROM} = \\frac{1}{s}\\sum_{i=1}^{s}\\frac{||\\mathbf{Q}(\\mu_i) - \\mathbf{Q}_{\\text{ROM}}(\\mu_i)||_F}{||\\mathbf{Q}(\\mu_i)||_F},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}_{\\text{ROM}}(\\mu_{i})$ is the ROM solution at parameter $\\mu_{i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test = np.sort(10**np.random.uniform(-1, 1, s))\n",
    "params_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial_parametric(r):\n",
    "    \"\"\"Do OpInf / intrusive ROM prediction with r basis vectors.\"\"\"\n",
    "    basis.r = r\n",
    "    q0_ = basis.encode(q0)\n",
    "\n",
    "    # Compute the intrusive ROM.\n",
    "    rom_intrusive = opinf.InterpolatedContinuousOpInfROM(\"AB\")\n",
    "    rom_intrusive.fit(basis, params, None, None,\n",
    "                      known_operators={\"A\": [µ * A for µ in params],\n",
    "                                       \"B\": [µ * B for µ in params]})\n",
    "\n",
    "    # Learn an operator inference ROM from the training data.\n",
    "    rom_opinf = opinf.InterpolatedContinuousOpInfROM(\"AB\")\n",
    "    rom_opinf.fit(basis, parameters=params, states=Qs_train, ddts=Qdots_train, inputs=Us_train)\n",
    "\n",
    "    # Test the ROM at each parameter in the test set.\n",
    "    projection_error, intrusive_error, opinf_error = 0, 0, 0\n",
    "    for µ in params_test:\n",
    "\n",
    "        # Solve the FOM at this parameter value.\n",
    "        Aµ = µ * A\n",
    "        Bµ = µ * B\n",
    "        Q_FOM = implicit_euler(t, q0, Aµ, Bµ, U_all)\n",
    "\n",
    "        # Simulate the intrusive ROM at this parameter value.\n",
    "        romµ = rom_intrusive(µ)\n",
    "        Q_ROM_intrusive = basis.decode(implicit_euler(t, q0_, romµ.A_.entries, romµ.B_.entries, U_all))\n",
    "\n",
    "        # Simulate the interpolating OpInf ROM at this parameter value.\n",
    "        romµ = rom_opinf(µ)\n",
    "        Q_ROM_opinf = basis.decode(implicit_euler(t, q0_, romµ.A_.entries, romµ.B_.entries, U_all))\n",
    "\n",
    "        # Calculate errors.\n",
    "        projection_error += basis.projection_error(Q_FOM, relative=True)\n",
    "        intrusive_error += opinf.post.frobenius_error(Q_FOM, Q_ROM_intrusive)[1]\n",
    "        opinf_error += opinf.post.frobenius_error(Q_FOM, Q_ROM_opinf)[1]\n",
    "\n",
    "    # Average the relative errors.\n",
    "    projection_error /= len(params_test)\n",
    "    intrusive_error /= len(params_test)\n",
    "    opinf_error /= len(params_test)\n",
    "\n",
    "    return projection_error, intrusive_error, opinf_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_error(14, run_trial_parametric, \"Average relative\\nFrobenius-norm error\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
